{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting complexPyTorch\n",
      "  Downloading complexPyTorch-0.4.tar.gz (9.8 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\jeane\\anaconda3\\lib\\site-packages (from complexPyTorch) (1.10.0+cpu)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jeane\\anaconda3\\lib\\site-packages (from torch->complexPyTorch) (3.7.4.2)\n",
      "Building wheels for collected packages: complexPyTorch\n",
      "  Building wheel for complexPyTorch (setup.py): started\n",
      "  Building wheel for complexPyTorch (setup.py): finished with status 'done'\n",
      "  Created wheel for complexPyTorch: filename=complexPyTorch-0.4-py3-none-any.whl size=7663 sha256=0fd9e0eda836d3577d431d5cd89b7cec72fd37fe764e3bb17aafd35b0ab1134b\n",
      "  Stored in directory: c:\\users\\jeane\\appdata\\local\\pip\\cache\\wheels\\df\\a5\\f7\\843d8ef45cc8317f4fc670427d4554f7646d4841fb08b8d362\n",
      "Successfully built complexPyTorch\n",
      "Installing collected packages: complexPyTorch\n",
      "Successfully installed complexPyTorch-0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install complexPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/wavefrontshaping/complexPyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from complexPyTorch.complexLayers import ComplexBatchNorm2d, ComplexConv2d, ComplexLinear\n",
    "from complexPyTorch.complexFunctions import complex_relu, complex_max_pool2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from collections import namedtuple, deque \n",
    "\n",
    "##Importing the model (function approximator for Q-table)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## équivalent à baselines/ddqn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##image_width représente le nombre d'action qu'on a\n",
    "##nos masques doivent être de la forme [batch_size, 1, max_width = 28], \n",
    "##nos kspace au niveau du ddqn seront de la forme [batch_size, coil = 1, max_width = 28, max_width,] ==>> on a + ib\n",
    "##au niveau de spirit on passera à  [batch_size, coil = 1, max_width = 28, max_width, 2] ##forme de départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##prendre un dico d'observation (recons , mask), et le fournit en input au qnetwork\n",
    "##\n",
    "def _encode_obs_myown(kspace_reconst, mask, img_reconst) :\n",
    "    \n",
    "    #transform into [batch_size, n_coils, height, width, 2]\n",
    "    kspace_reconst = torch.cat( (torch.unsqueeze(kspace_reconst.real, -1),torch.unsqueeze(kspace_reconst.imag, -1) ),-1) #shape [n, coil = 1, max_width = 28, max_width, 2]\n",
    "    \n",
    "    return {\"kspace_reconst\", \"mask\", \"img_reconst\"}\n",
    "\n",
    "\n",
    "def _decode_obs_dict_myown(obs: Dict[str, Any]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "   ## obs[\"kspace_reconst\"] a la forme [batch_size, num_channels, img_height, img_width, 2]\n",
    "    kspace_reconst = obs[\"kspace_reconst\"][...,0] +1j*obs[\"kspace_reconst\"][...,1]\n",
    "    \n",
    "    mask = obs[\"mask\"]\n",
    "    img_reconst = obs[\"img_reconst\"]\n",
    "    bs = obs_tensor.shape[0] #batch_size\n",
    "\n",
    "    mask = obs_tensor[:, 0, -1, :] ##ceci reviendra dont à \n",
    "    mask = mask.contiguous().view(bs, 1, 1, -1) #(batch_size, 1, 1, img_width)\n",
    "\n",
    "    return kspace_reconst, mask, img_reconst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ce q network va prendre en entrée des kspace à valeurs complexes et faire sortir les q-values des actions\n",
    "\n",
    "##l'architecture du modèle utilié vient de : https://github.com/wavefrontshaping/complexPyTorch\n",
    "\n",
    "class q_ComplexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, opts, ):\n",
    "        \n",
    "        super(q_ComplexNet, self).__init__()\n",
    "        self.conv1 = ComplexConv2d(1, 10, 5, 1)\n",
    "        self.bn  = ComplexBatchNorm2d(10)\n",
    "        self.conv2 = ComplexConv2d(10, 20, 5, 1)\n",
    "        self.fc1 = ComplexLinear(4*4*20, 500)\n",
    "        self.fc2 = ComplexLinear(500, opts.image_width)\n",
    "             \n",
    "    def forward(self, obs: Dict[str, Any]) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"Predicts action values.\n",
    "        Args:\n",
    "            obs(torch.Tensor): The observation tensor. Once decoded, it only uses the mask\n",
    "                               information. If ``__init__(..., ignore_mask=True)``, it will\n",
    "                               additionally use the mask only to deduce the time step.\n",
    "        Returns:\n",
    "            torch.Tensor: Q-values for all actions at the given observation.\n",
    "        Note:\n",
    "            Values corresponding to active k-space columns in the observation are manually\n",
    "            set to ``1e-10``.\n",
    "        \"\"\"\n",
    "        \n",
    "        kspace_reconst, mask, _ = _decode_obs_dict_myown(obs ## il faudrait faire en sorte que l'état st (observation) soit un dico compotant le kspace reconstruit et le mask\n",
    "        previous_actions = mask.squeeze() ##??? [batch_size, width = 28 ]\n",
    "        \n",
    "        input_tensor = kspace_reconst ## ils ont  mis mask dans le cas data-specific\n",
    "\n",
    "        \n",
    "        x = self.conv1(input_tensor)\n",
    "        x = complex_relu(x)\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = self.bn(x)\n",
    "        x = self.conv2(x)\n",
    "        x = complex_relu(x)\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1,4*4*20)\n",
    "        x = self.fc1(x)\n",
    "        x = complex_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        value = x.abs()  ### est ce que ça a de sens ???? Pour le moment, mon objectif est d'avoir un modèle qui a une démarche de RL, après on peut enlever ce qui ne marche pas.\n",
    "        \n",
    "\n",
    "        return value - 1e10 * previous_actions \n",
    "\n",
    "\"\"\"a = torch.randn((4,1,28,28,2))\n",
    "\n",
    "z = a[...,0]+1j*b[...,1]\n",
    "\n",
    "dd = q_ComplexNet()\n",
    "so = dd(z)\n",
    "\n",
    "so.shape\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model(options):\n",
    "    if options.dqn_model_type == \"q_ComplexNet\":\n",
    "        return q_ComplexNet(options)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unknown model specified for DDQN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###remplacer posiblement options par un argparse, cet l'équivalent d'un config\n",
    "options = {\"BUFFER_SIZE\": int(1e5),  #replay buffer size\n",
    "          \"BATCH_SIZE\": 64,   # minibatch size  \n",
    "          \"GAMMA\": 0.5,       # discount factor\n",
    "          \"TAU\":1e-3,         # for soft update of target parameters\n",
    "           \"LR\":5e-4,         # learning rate\n",
    "          \"UPDATE_EVERY\":4,   # how often to update the network\n",
    "          \n",
    "          \"dqn_model_type\": \"q_ComplexNet\",\n",
    "          \"budget\":5, ##je veux prendre 2 colonnes au départ et le modèle va déterminer les 5 restants ; on  a au total 7 colonnes, soit une accélération de 4\n",
    "          \"image_width\":28, ## it is the action_size\n",
    "          \"seed\" : 123}\n",
    "\n",
    "\n",
    "class DDQN():\n",
    "    \"\"\"Interacts with and learns form environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, options):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        =======\n",
    "            options\n",
    "        \"\"\"        \n",
    "\n",
    "        self.options = options\n",
    "        self.seed = random.seed(options.seed)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Q- Network\n",
    "        self.qnetwork_local = _get_model(options).to(device)\n",
    "        self.qnetwork_target = _get_model(options).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr = options.LR)\n",
    "        \n",
    "        # Replay memory \n",
    "        self.memory = ReplayBuffer(self.options.image_width, self.options.BUFFER_SIZE, self.options.BATCH_SIZE, self.options.seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        \n",
    "        self.t_step = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    def step(self, state, action, reward, next_step, done): ##state here is observation  dict, next_step is the same thing\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_step, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step+1)% self.options.UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "\n",
    "            if len(self.memory)>self.options.BATCH_SIZE:\n",
    "                experience = self.memory.sample()\n",
    "                self.learn(experience, self.options.GAMMA)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def act(self, state, eps = 0):\n",
    "        \"\"\"Returns action for given state as per current policy\n",
    "        Params\n",
    "        =======\n",
    "            state : kspace_reconst, mask, img_reconst\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "                \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            action_values = self.qnetwork_local(state)\n",
    "            \n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #Epsilon -greedy action selction ; en prenant l'action, il faut modifier le masque en conséquence\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            \n",
    "            \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        =======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples ; with s an observation dico\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_state, dones = experiences\n",
    "        ## TODO: compute and minimize the loss\n",
    "        criterion = torch.nn.SmoothL1Loss()\n",
    "        self.qnetwork_local.train()\n",
    "        self.qnetwork_target.eval()\n",
    "        \n",
    "        #shape of output from the model (batch_size,action_dim) = (batch_size,28)\n",
    "        predicted_targets = self.qnetwork_local(states).gather(1,actions)\n",
    "\n",
    "        #################Updates for Double DQN learning###########################\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions_q_local = self.qnetwork_local(next_states).detach().max(1)[1].unsqueeze(1).long()\n",
    "            labels_next = self.qnetwork_target(next_states).gather(1,actions_q_local)\n",
    "        self.qnetwork_local.train()\n",
    "        ############################################################################\n",
    "\n",
    "        # .detach() ->  Returns a new Tensor, detached from the current graph.\n",
    "        labels = rewards + (gamma* labels_next*(1-dones))\n",
    "\n",
    "        loss = criterion(predicted_targets,labels).to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local,self.qnetwork_target,self.options.TAU)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        =======\n",
    "            local model (PyTorch model): weights will be copied from\n",
    "            target model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "            \n",
    "            \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed -size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, options, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.options = options\n",
    "        self.action_size = self.options.image_width\n",
    "        self.memory = deque(maxlen=self.options.BUFFER_SIZE)\n",
    "        self.batch_size = self.options.BATCH_SIZE\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experiences(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "        \n",
    "    def concat_dict_element(dict_sample,dim=0) : \n",
    "    \"\"\"concat the tensor that are in different state dictionnary or next_state\"\"\"\n",
    "    dico = {}\n",
    "    for key in dict_sample[0].keys():\n",
    "        dico[key] = torch.cat( [ dic[key] for dic in dict_sample ], 0).to(device) #.float .complex ?\n",
    "     \n",
    "    return dico ##this dico will contain a batch of kspace reconstruction, a batch of mask and a batch of image reconstruction\n",
    "    \n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        ##take an experience and extract the keys that are in the state dictionnary\n",
    "        \n",
    "        \n",
    "        states = concat_dict_element([e.state for e experiences if e is not None],dim=0)\n",
    "        \n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        \n",
    "        next_states = concat_dict_element([e.next_state for e experiences if e is not None],dim=0)\n",
    "        \n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dqn(n_episodes= 200, max_t = 1000, eps_start=1.0, eps_end = 0.01,\n",
    "       eps_decay=0.996):\n",
    "    \"\"\"Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training epsiodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon \n",
    "        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    scores = [] # list containing score from each episode\n",
    "    scores_window = deque(maxlen=100) # last 100 scores\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "            ## above step decides whether we will train(learn) the network\n",
    "            ## actor (local_qnetwork) or we will fill the replay buffer\n",
    "            ## if len replay buffer is equal to the batch size then we will\n",
    "            ## train the network or otherwise we will add experience tuple in our \n",
    "            ## replay buffer.\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            scores_window.append(score) ## save the most recent score\n",
    "            scores.append(score) ## sae the most recent score\n",
    "            eps = max(eps*eps_decay,eps_end)## decrease the epsilon\n",
    "            print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode,np.mean(scores_window)), end=\"\")\n",
    "            if i_episode %100==0:\n",
    "                print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode,np.mean(scores_window)))\n",
    "                \n",
    "            if np.mean(scores_window)>=200.0:\n",
    "                print('\\nEnvironment solve in {:d} epsiodes!\\tAverage score: {:.2f}'.format(i_episode-100,\n",
    "                                                                                           np.mean(scores_window)))\n",
    "                torch.save(agent.qnetwork_local.state_dict(),'checkpoint.pth')\n",
    "                break\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    \"\"\"DDQN Trainer for active MRI acquisition.\n",
    "    Configuration for the trainer is provided by argument ``options``. Must contain the\n",
    "    following fields:\n",
    "        - \"checkpoints_dir\"(str): The directory where the model will be saved to (or\n",
    "          loaded from).\n",
    "        - dqn_batch_size(int): The batch size to use for updates.\n",
    "        - dqn_burn_in(int): How many steps to do before starting updating parameters.\n",
    "        - dqn_normalize(bool): ``True`` if running mean/st. deviation should be maintained\n",
    "          for observations.\n",
    "        - dqn_only_test(bool): ``True`` if the model will not be trained, thus only will\n",
    "          attempt to read from checkpoint and load only weights of the network (ignoring\n",
    "          training related information).\n",
    "        - dqn_test_episode_freq(optional(int)): How frequently (in number of env steps)\n",
    "          to perform test episodes.\n",
    "        - freq_dqn_checkpoint_save(int): How often (in episodes) to save the model.\n",
    "        - num_train_steps(int): How many environment steps to train for.\n",
    "        - replay_buffer_size(int): The capacity of the replay buffer.\n",
    "        - resume(bool): If true, will try to load weights from the checkpoints dir.\n",
    "        - num_test_episodes(int): How many test episodes to periodically evaluate for.\n",
    "        - seed(int): Sets the seed for the environment when running evaluation episodes.\n",
    "        - reward_metric(str): Which of the ``env.scores_keys()`` is used as reward. Mainly\n",
    "          used for logging purposes.\n",
    "        - target_net_update_freq(int): How often (in env's steps) to update the target\n",
    "          network.\n",
    "    Args:\n",
    "        options(``argparse.Namespace``): Options for the trainer.\n",
    "        env(``activemri.envs.ActiveMRIEnv``): Env for which the policy is trained.\n",
    "        device(``torch.device``): Device to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        options: argparse.Namespace,\n",
    "        env: mri_envs.ActiveMRIEnv,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.options = options\n",
    "        self.env = env\n",
    "        self.options.image_width = self.env.kspace_width\n",
    "        self.steps = 0\n",
    "        self.episode = 0\n",
    "        self.best_test_score = -np.inf\n",
    "        self.device = device\n",
    "        self.replay_memory = None\n",
    "\n",
    "        self.window_size = 1000\n",
    "        self.reward_images_in_window = np.zeros(self.window_size)\n",
    "        self.current_score_auc_window = np.zeros(self.window_size)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Creating DDQN model.\")\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Creating replay buffer with capacity {options.mem_capacity}.\"\n",
    "        )\n",
    "\n",
    "        # ------- Create replay buffer and networks ------\n",
    "        # See _encode_obs_dict() for tensor format\n",
    "        self.obs_shape = (2, self.env.kspace_height + 2, self.env.kspace_width)\n",
    "        self.replay_memory = replay_buffer.ReplayMemory(\n",
    "            options.mem_capacity,\n",
    "            self.obs_shape,\n",
    "            self.options.dqn_batch_size,\n",
    "            self.options.dqn_burn_in,\n",
    "            \n",
    "        )\n",
    "        print(\"Created replay buffer.\")\n",
    "        \n",
    "        self.policy = DDQN(device, self.replay_memory, self.options)\n",
    "        self.target_net = DDQN(device, None, self.options)\n",
    "        self.target_net.eval()\n",
    "        self.logger.info(\n",
    "            f\"Created neural networks with {self.env.action_space.n} outputs.\"\n",
    "        )\n",
    "\n",
    "        # ------- Files used to communicate with DDQNTester ------\n",
    "        self.folder_lock_path = DDQNTrainer.get_lock_filename(\n",
    "            self.options.checkpoints_dir\n",
    "        )\n",
    "        with _get_folder_lock(self.folder_lock_path):\n",
    "            # Write options so that tester can read them\n",
    "            with open(\n",
    "                DDQNTrainer.get_options_filename(self.options.checkpoints_dir), \"wb\"\n",
    "            ) as f:\n",
    "                pickle.dump(self.options, f)\n",
    "            # Remove previous done file since training will start over\n",
    "            done_file = DDQNTrainer.get_done_filename(self.options.checkpoints_dir)\n",
    "            if os.path.isfile(done_file):\n",
    "                os.remove(done_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_done_filename(path):\n",
    "        return os.path.join(path, \"DONE\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_name_latest_checkpoint(path):\n",
    "        return os.path.join(path, \"policy_checkpoint.pth\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_options_filename(path):\n",
    "        return os.path.join(path, \"options.pickle\")\n",
    "\n",
    "    def _max_replay_buffer_size(self):\n",
    "        return min(self.options.num_train_steps, self.options.replay_buffer_size)\n",
    "\n",
    "    def load_checkpoint_if_needed(self):\n",
    "        if self.options.dqn_only_test or self.options.resume:\n",
    "            policy_path = os.path.join(self.options.dqn_weights_path)\n",
    "            if os.path.isfile(policy_path):\n",
    "                self.load(policy_path)\n",
    "                self.logger.info(f\"Loaded DQN policy found at {policy_path}.\")\n",
    "            else:\n",
    "                self.logger.warning(f\"No DQN policy found at {policy_path}.\")\n",
    "                if self.options.dqn_only_test:\n",
    "                    raise FileNotFoundError\n",
    "\n",
    "    def _train_dqn_policy(self):\n",
    "        \"\"\" Trains the DQN policy. \"\"\"\n",
    "        print(\n",
    "            f\"Starting training at step {self.steps}/{self.options.num_train_steps}. \"\n",
    "            f\"Best score so far is {self.best_test_score}.\"\n",
    "        )\n",
    "\n",
    "        steps_epsilon = self.steps\n",
    "        while self.steps < self.options.num_train_steps:\n",
    "            self.logger.info(\"Episode {}\".format(self.episode + 1))\n",
    "\n",
    "            # Evaluate the current policy\n",
    "            if self.options.dqn_test_episode_freq and (\n",
    "                self.episode % self.options.dqn_test_episode_freq == 0\n",
    "            ):\n",
    "                \n",
    "                test_scores, _ = evaluation.evaluate(\n",
    "                    self.env,\n",
    "                    self.policy,\n",
    "                    self.options.num_test_episodes,\n",
    "                    self.options.seed,\n",
    "                    \"val\",\n",
    "                )\n",
    "                \n",
    "                self.env.set_training()\n",
    "                auc_score = test_scores[self.options.reward_metric].sum(axis=1).mean()\n",
    "                if \"mse\" in self.options.reward_metric:\n",
    "                    auc_score *= -1\n",
    "                if auc_score > self.best_test_score:\n",
    "                    policy_path = os.path.join(\n",
    "                        self.options.checkpoints_dir, \"policy_best.pt\"\n",
    "                    )\n",
    "                    self.save(policy_path)\n",
    "                    self.best_test_score = auc_score\n",
    "                    self.logger.info(\n",
    "                        f\"Saved DQN model with score {self.best_test_score} to {policy_path}.\"\n",
    "                    )\n",
    "\n",
    "            # Save model periodically\n",
    "            if self.episode % self.options.freq_dqn_checkpoint_save == 0:\n",
    "                self.checkpoint(save_memory=False)\n",
    "\n",
    "            # Run an episode and update model\n",
    "            obs, meta = self.env.reset()\n",
    "            msg = \", \".join(\n",
    "                [\n",
    "                    f\"({meta['fname'][i]}, {meta['slice_id'][i]})\"\n",
    "                    for i in range(len(meta[\"slice_id\"]))\n",
    "                ]\n",
    "            )\n",
    "            self.logger.info(f\"Episode started with images {msg}.\")\n",
    "            all_done = False\n",
    "            total_reward = 0\n",
    "            auc_score = 0\n",
    "            while not all_done:\n",
    "                epsilon = _get_epsilon(steps_epsilon, self.options)\n",
    "                action = self.policy.get_action(obs, eps_threshold=epsilon)\n",
    "                next_obs, reward, done, meta = self.env.step(action)\n",
    "                auc_score += meta[\"current_score\"][self.options.reward_metric]\n",
    "                all_done = all(done)\n",
    "                self.steps += 1\n",
    "                obs_tensor = _encode_obs_dict(obs)\n",
    "                next_obs_tensor = _encode_obs_dict(next_obs)\n",
    "                batch_size = len(obs_tensor)\n",
    "                for i in range(batch_size):\n",
    "                    self.policy.add_experience(\n",
    "                        obs_tensor[i], action[i], next_obs_tensor[i], reward[i], done[i]\n",
    "                    )\n",
    "\n",
    "                update_results = self.policy.update_parameters(self.target_net)\n",
    "                torch.cuda.empty_cache()\n",
    "                if self.steps % self.options.target_net_update_freq == 0:\n",
    "                    self.logger.info(\"Updating target network.\")\n",
    "                    self.target_net.load_state_dict(self.policy.state_dict())\n",
    "                steps_epsilon += 1\n",
    "\n",
    "                # Adding per-step tensorboard logs\n",
    "                if self.steps % 250 == 0:\n",
    "                    self.logger.debug(\"Writing to tensorboard.\")\n",
    "                    self.writer.add_scalar(\"epsilon\", epsilon, self.steps)\n",
    "                    if update_results is not None:\n",
    "                        self.writer.add_scalar(\n",
    "                            \"loss\", update_results[\"loss\"], self.steps\n",
    "                        )\n",
    "                        self.writer.add_scalar(\n",
    "                            \"grad_norm\", update_results[\"grad_norm\"], self.steps\n",
    "                        )\n",
    "                        self.writer.add_scalar(\n",
    "                            \"mean_q_value\", update_results[\"q_values_mean\"], self.steps\n",
    "                        )\n",
    "                        self.writer.add_scalar(\n",
    "                            \"std_q_value\", update_results[\"q_values_std\"], self.steps\n",
    "                        )\n",
    "\n",
    "                total_reward += reward\n",
    "                obs = next_obs\n",
    "\n",
    "            # Adding per-episode tensorboard logs\n",
    "            total_reward = total_reward.mean().item()\n",
    "            auc_score = auc_score.mean().item()\n",
    "            self.reward_images_in_window[self.episode % self.window_size] = total_reward\n",
    "            self.current_score_auc_window[self.episode % self.window_size] = auc_score\n",
    "            self.writer.add_scalar(\"episode_reward\", total_reward, self.episode)\n",
    "            self.writer.add_scalar(\n",
    "                \"average_reward_images_in_window\",\n",
    "                np.sum(self.reward_images_in_window)\n",
    "                / min(self.episode + 1, self.window_size),\n",
    "                self.episode,\n",
    "            )\n",
    "            self.writer.add_scalar(\n",
    "                \"average_auc_score_in_window\",\n",
    "                np.sum(self.current_score_auc_window)\n",
    "                / min(self.episode + 1, self.window_size),\n",
    "                self.episode,\n",
    "            )\n",
    "\n",
    "            self.episode += 1\n",
    "\n",
    "        self.checkpoint()\n",
    "\n",
    "        # Writing DONE file with best test score\n",
    "        with _get_folder_lock(self.folder_lock_path):\n",
    "            with open(\n",
    "                DDQNTrainer.get_done_filename(self.options.checkpoints_dir), \"w\"\n",
    "            ) as f:\n",
    "                f.write(str(self.best_test_score))\n",
    "\n",
    "        return self.best_test_score\n",
    "\n",
    "    def __call__(self):\n",
    "        self.load_checkpoint_if_needed()\n",
    "        return self._train_dqn_policy()\n",
    "\n",
    "    def checkpoint(self, save_memory=True):\n",
    "        policy_path = DDQNTrainer.get_name_latest_checkpoint(\n",
    "            self.options.checkpoints_dir\n",
    "        )\n",
    "        self.save(policy_path)\n",
    "        self.logger.info(f\"Saved DQN checkpoint to {policy_path}\")\n",
    "        if save_memory:\n",
    "            self.logger.info(\"Now saving replay memory.\")\n",
    "            memory_path = self.replay_memory.save(\n",
    "                self.options.checkpoints_dir, \"replay_buffer.pt\"\n",
    "            )\n",
    "            self.logger.info(f\"Saved replay buffer to {memory_path}.\")\n",
    "\n",
    "    def save(self, path):\n",
    "        with _get_folder_lock(self.folder_lock_path):\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"dqn_weights\": self.policy.state_dict(),\n",
    "                    \"target_weights\": self.target_net.state_dict(),\n",
    "                    \"options\": self.options,\n",
    "                    \"episode\": self.episode,\n",
    "                    \"steps\": self.steps,\n",
    "                    \"best_test_score\": self.best_test_score,\n",
    "                    \"reward_images_in_window\": self.reward_images_in_window,\n",
    "                    \"current_score_auc_window\": self.current_score_auc_window,\n",
    "                },\n",
    "                path,\n",
    "            )\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy.load_state_dict(checkpoint[\"dqn_weights\"])\n",
    "        self.episode = checkpoint[\"episode\"] + 1\n",
    "        if not self.options.dqn_only_test:\n",
    "            self.target_net.load_state_dict(checkpoint[\"target_weights\"])\n",
    "            self.steps = checkpoint[\"steps\"]\n",
    "            self.best_test_score = checkpoint[\"best_test_score\"]\n",
    "            self.reward_images_in_window = checkpoint[\"reward_images_in_window\"]\n",
    "            self.current_score_auc_window = checkpoint[\"current_score_auc_window\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=8,action_size=4,seed=0)\n",
    "\n",
    "scores= dqn()\n",
    "\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3956,  0.1844,  0.3374, -0.5964,  0.1253],\n",
       "          [-0.4805,  1.1258,  0.2264,  1.2968, -2.7566],\n",
       "          [ 0.6585, -1.4463, -0.5388,  0.6482,  0.0750],\n",
       "          [-0.7434, -0.1443,  0.2038,  0.7724,  0.7934],\n",
       "          [-1.4173, -1.3582,  0.1742,  0.4491,  0.7307]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8428, -1.7390,  1.0562,  2.1782, -1.5747],\n",
       "          [-0.2892, -1.9416,  0.0620, -0.3615,  0.4342],\n",
       "          [ 0.1588, -0.1162, -1.3501,  0.9667,  2.2290],\n",
       "          [ 0.7906, -0.4644, -0.0308,  0.3798, -1.2911],\n",
       "          [-0.7894,  0.8741,  2.8280,  0.1516, -1.6244]]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((2,1,5,5))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 1.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(np.expand_dims(np.eye(5), (0,1)))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.expand_dims(np.fliplr(np.eye(5)) , (0,1))\n",
    "c = torch.from_numpy(c.copy())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.cat((b,c),0)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10000000.,        0.,        0.,        0.,        0.],\n",
       "          [       0., 10000000.,        0.,        0.,        0.],\n",
       "          [       0.,        0., 10000000.,        0.,        0.],\n",
       "          [       0.,        0.,        0., 10000000.,        0.],\n",
       "          [       0.,        0.,        0.,        0., 10000000.]]],\n",
       "\n",
       "\n",
       "        [[[       0.,        0.,        0.,        0., 10000000.],\n",
       "          [       0.,        0.,        0., 10000000.,        0.],\n",
       "          [       0.,        0., 10000000.,        0.,        0.],\n",
       "          [       0., 10000000.,        0.,        0.,        0.],\n",
       "          [10000000.,        0.,        0.,        0.,        0.]]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = d*1e7\n",
    "e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000e+07,  1.8441e-01,  3.3739e-01, -5.9643e-01,  1.2526e-01],\n",
       "          [-4.8053e-01,  1.0000e+07,  2.2641e-01,  1.2968e+00, -2.7566e+00],\n",
       "          [ 6.5847e-01, -1.4463e+00,  1.0000e+07,  6.4818e-01,  7.5019e-02],\n",
       "          [-7.4336e-01, -1.4428e-01,  2.0379e-01,  1.0000e+07,  7.9336e-01],\n",
       "          [-1.4173e+00, -1.3582e+00,  1.7416e-01,  4.4911e-01,  1.0000e+07]]],\n",
       "\n",
       "\n",
       "        [[[ 8.4277e-01, -1.7390e+00,  1.0562e+00,  2.1782e+00,  1.0000e+07],\n",
       "          [-2.8924e-01, -1.9416e+00,  6.2042e-02,  1.0000e+07,  4.3420e-01],\n",
       "          [ 1.5880e-01, -1.1625e-01,  1.0000e+07,  9.6666e-01,  2.2290e+00],\n",
       "          [ 7.9059e-01,  1.0000e+07, -3.0757e-02,  3.7985e-01, -1.2911e+00],\n",
       "          [ 1.0000e+07,  8.7413e-01,  2.8280e+00,  1.5158e-01, -1.6244e+00]]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_bis = torch.where(d==1, torch.tensor(1e7, dtype =a.dtype), a)\n",
    "a_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1, dtype =a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=2\n",
    "a *=4\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "a = False\n",
    "\n",
    "if  not a:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "d = edict({})\n",
    "d[str(1)] = 2\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.r =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 2, 'r': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "d[1] = \"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
